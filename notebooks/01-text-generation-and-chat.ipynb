{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samay58/prompt-playground/blob/main/notebooks/01-text-generation-and-chat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc71dad6",
      "metadata": {
        "id": "dc71dad6"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/philschmid/gemini-2.5-ai-engineering-workshop/blob/main/notebooks/01-text-generation-and-chat.ipynb)\n",
        "\n",
        "# Part 1 - Text Generation and Chat\n",
        "\n",
        "This part focuses on text generation with the Gemini API using the `google-genai` SDK, including basic prompts, chat interactions, streaming, and configuration.\n",
        "\n",
        "Make sure you have completed the [setup and authentication](solution_00_setup_and_authentication.md) section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "17fe7909",
      "metadata": {
        "id": "17fe7909"
      },
      "outputs": [],
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "import os\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    from google.colab import userdata\n",
        "    GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "else:\n",
        "    GEMINI_API_KEY = os.environ.get('GEMINI_API_KEY',None)\n",
        "\n",
        "# Create client with api key\n",
        "MODEL_ID = \"gemini-2.5-flash-preview-05-20\"\n",
        "client = genai.Client(api_key=GEMINI_API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e25baada",
      "metadata": {
        "id": "e25baada"
      },
      "source": [
        "## 1. Send Your First Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d19f5446",
      "metadata": {
        "id": "d19f5446",
        "outputId": "29b1e29b-2df5-4dbc-dd9d-c6bead2ca5f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response from Gemini:\n",
            "Here are 3 names for a new coffee shop emphasizing sustainability:\n",
            "\n",
            "1.  **Root & Bloom Cafe**\n",
            "    *   **Emphasis:** \"Root\" signifies ethical sourcing, the origin of the coffee bean, and a strong foundation in sustainable practices. \"Bloom\" suggests growth, flourishing, and the vibrant life that sustainable practices support, including the coffee plant itself.\n",
            "    *   **Vibe:** Natural, earthy, positive, growth-oriented.\n",
            "\n",
            "2.  **The Seedling Coffee Co.**\n",
            "    *   **Emphasis:** \"Seedling\" evokes new beginnings, nurturing, and the potential for future growth, directly linking to environmental stewardship and planting for the future. It's a tangible symbol of sustainability. \"Coffee Co.\" adds a modern, artisanal touch.\n",
            "    *   **Vibe:** Fresh, optimistic, forward-thinking, community-focused.\n",
            "\n",
            "3.  **Auraverde Brews**\n",
            "    *   **Emphasis:** \"Aura\" implies a positive atmosphere, a feeling, or a distinctive quality. \"Verde\" (Spanish for green) directly references environmentalism and nature. Together, it suggests a coffee shop with a distinctly green, positive, and sustainable essence. \"Brews\" keeps it clear that it's a coffee shop.\n",
            "    *   **Vibe:** Unique, eco-chic, atmospheric, globally-conscious.\n"
          ]
        }
      ],
      "source": [
        "prompt = \"Create 3 names for a new coffee shop that emphasizes sustainability.\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt\n",
        ")\n",
        "\n",
        "print(\"Response from Gemini:\")\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1271d094",
      "metadata": {
        "id": "1271d094"
      },
      "source": [
        "#### **!! Exercise: Sending Various Prompts !!**\n",
        "\n",
        "Practice sending different types of prompts to the Gemini model and observe its responses. You can also experiment with different model versions if they are available to you.\n",
        "\n",
        "Tasks:\n",
        "- Write a prompt to ask Gemini to generate a short poem about a robot.\n",
        "- Write a prompt to ask Gemini to explain \"machine learning\" in simple terms.\n",
        "- Try other models (e.g., `gemini-2.0-flash`) and send your prompts to them and compare the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "6329dfcb",
      "metadata": {
        "id": "6329dfcb",
        "outputId": "5be28c8e-afcc-4a20-a95b-1fba11a134a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response from Gemini:\n",
            "Small bot, wheels deep in rust-red sand,\n",
            "Alone on Mars, a silent land.\n",
            "His optic lens, a curious glow,\n",
            "Where only Martian breezes blow.\n",
            "\n",
            "He polishes pebbles, smooth and round,\n",
            "And hums a happy, whirring sound.\n",
            "Maps ancient craters with a whir,\n",
            "His tiny antennae start to stir.\n",
            "\n",
            "No distant signal, no kind word,\n",
            "Just cosmic silence, rarely stirred.\n",
            "Yet in this vast and lonely grace,\n",
            "He finds his purpose, finds his place.\n",
            "\n",
            "Each red sunset, a brand new hue,\n",
            "More Martian dust to roll right through.\n",
            "The cutest rover, bright and keen,\n",
            "On Mars, the happiest machine.\n",
            "Response from Gemini:\n",
            "Okay, imagine you have a very curious friend, let's call them \"Computer.\"\n",
            "\n",
            "**The Big Idea: Teaching Computers to Learn from Experience**\n",
            "\n",
            "Normally, when you want a computer to do something, you have to give it very specific, step-by-step instructions (that's called \"programming\"). For example, \"If the light is red, stop. If the light is green, go.\"\n",
            "\n",
            "But what if you want the computer to recognize a cat in a photo? How would you write instructions for *every single possible cat photo*? A cat from the front, side, lying down, different colors, different lighting... It would be impossible!\n",
            "\n",
            "**Machine learning (ML) is a way of teaching computers to learn from data, just like humans learn from experience, instead of being explicitly programmed for every single task.**\n",
            "\n",
            "Think of it like this:\n",
            "\n",
            "---\n",
            "\n",
            "### The Analogy: Teaching a Child to Recognize Dogs\n",
            "\n",
            "1.  **The Goal:** You want your friend (the computer) to be able to tell the difference between a picture of a dog and a picture that *isn't* a dog.\n",
            "\n",
            "2.  **The \"Data\" (Experience):**\n",
            "    *   You show the child **hundreds, maybe thousands, of pictures**.\n",
            "    *   For each picture, you say, \"This IS a dog\" or \"This is NOT a dog.\"\n",
            "    *   You point out things: \"See, dogs often have four legs, a tail, fur, and a specific shape to their face.\"\n",
            "    *   Sometimes the child might make a mistake (\"Is that a dog?\"). You correct them (\"No, that's a sheep; notice the different type of fur and face\").\n",
            "\n",
            "3.  **The \"Learning\" (Training):**\n",
            "    *   Over time, the child starts to **find patterns**. They realize that fuzzy animals with wagging tails and pointy ears are *usually* dogs, while scaly animals are *never* dogs.\n",
            "    *   They build a kind of \"internal rulebook\" in their head about what makes a dog a dog. They don't write down every single rule, but they *learn* the common characteristics.\n",
            "\n",
            "4.  **The \"Prediction\" (Using the Knowledge):**\n",
            "    *   Now, you show the child a **brand new picture** they've never seen before.\n",
            "    *   They look at it, compare it to all the patterns they've learned, and confidently say, \"That IS a dog!\" or \"That is NOT a dog!\"\n",
            "    *   They've *learned* to make a good guess based on their experience.\n",
            "\n",
            "---\n",
            "\n",
            "### How Machine Learning Works (For Computers):\n",
            "\n",
            "1.  **Data (The Fuel):**\n",
            "    *   Instead of pictures for a child, for a computer, data can be:\n",
            "        *   Thousands of images (labeled \"cat\" or \"not cat\")\n",
            "        *   Recordings of human speech (labeled \"hello,\" \"weather,\" etc.)\n",
            "        *   Past customer purchases (labeled \"bought item X,\" \"didn't buy item X\")\n",
            "        *   Medical scans (labeled \"tumor\" or \"no tumor\")\n",
            "    *   **Relevant Detail:** The more high-quality, relevant data you have, the better the machine learning model will learn. This data is the \"experience.\"\n",
            "\n",
            "2.  **Algorithm (The \"Learning Method\" or \"Recipe\"):**\n",
            "    *   This is the special program or mathematical \"recipe\" that the computer uses to learn from the data.\n",
            "    *   Think of it as the specific way the child tries to find patterns. Some algorithms are better for images, some for text, some for making predictions.\n",
            "    *   **Relevant Detail:** There are many different types of machine learning algorithms (like Linear Regression, Decision Trees, Neural Networks), each with its own strengths.\n",
            "\n",
            "3.  **Training (The Learning Phase):**\n",
            "    *   You feed the data into the algorithm.\n",
            "    *   The algorithm \"studies\" the data, looking for patterns, relationships, and rules.\n",
            "    *   It's like the child looking at pictures and figuring out what makes a dog.\n",
            "    *   The computer adjusts its internal \"rules\" (called **parameters** or **weights**) over and over, trying to get better at making correct predictions.\n",
            "    *   **Relevant Detail:** This phase can take a lot of time and powerful computers, especially with huge amounts of data.\n",
            "\n",
            "4.  **Model (The \"Learned Brain\"):**\n",
            "    *   Once the training is done, the computer has created a \"model.\"\n",
            "    *   This model is the \"internal rulebook\" or \"knowledge base\" that the computer has built. It's the equivalent of the child's learned ability to recognize a dog.\n",
            "    *   **Relevant Detail:** The model isn't just a list of \"if-then\" statements; it's a complex mathematical representation of the patterns it found in the data.\n",
            "\n",
            "5.  **Prediction / Inference (Using the Model):**\n",
            "    *   Now, you can give the trained model *new, unseen data*.\n",
            "    *   The model uses its learned patterns to make a guess or decision based on what it learned.\n",
            "    *   \"Is this new email spam or not spam?\"\n",
            "    *   \"What word did the user just say?\"\n",
            "    *   \"Will this customer click on this ad?\"\n",
            "\n",
            "---\n",
            "\n",
            "### Why is Machine Learning So Useful? (Applications)\n",
            "\n",
            "*   **Recommendations:** Netflix suggesting movies, Spotify suggesting music, Amazon suggesting products. (\"People who watched X also liked Y.\")\n",
            "*   **Voice Assistants:** Siri, Alexa, Google Assistant understanding what you say.\n",
            "*   **Image Recognition:** Tagging friends in photos, identifying objects, facial recognition.\n",
            "*   **Spam Filters:** Recognizing junk email.\n",
            "*   **Medical Diagnosis:** Helping doctors find diseases in scans.\n",
            "*   **Self-Driving Cars:** Learning to recognize roads, signs, pedestrians, and other cars.\n",
            "*   **Fraud Detection:** Banks spotting unusual transactions that might be fake.\n",
            "*   **Language Translation:** Google Translate.\n",
            "\n",
            "---\n",
            "\n",
            "### Key Types of Machine Learning (Briefly):\n",
            "\n",
            "1.  **Supervised Learning:**\n",
            "    *   This is the \"teaching a child with a teacher\" example.\n",
            "    *   The data you train with is **labeled** (e.g., \"This *is* a dog,\" \"This *is not* a dog\").\n",
            "    *   The goal is to predict a specific outcome.\n",
            "\n",
            "2.  **Unsupervised Learning:**\n",
            "    *   Imagine you just give the child a huge pile of pictures and say, \"Find groups of similar pictures!\"\n",
            "    *   The data is **unlabeled**. The computer has to find patterns and structures on its own.\n",
            "    *   Example: Grouping customers by their purchasing habits without being told what those groups should be.\n",
            "\n",
            "3.  **Reinforcement Learning:**\n",
            "    *   Think of training a pet with rewards.\n",
            "    *   The computer learns by trial and error, performing actions in an environment and getting \"rewards\" for good actions and \"penalties\" for bad ones.\n",
            "    *   Example: Teaching a computer to play a video game, where scoring points is a reward.\n",
            "\n",
            "---\n",
            "\n",
            "### Important Considerations:\n",
            "\n",
            "*   **\"Garbage In, Garbage Out\":** If the data used for training is poor, biased, or incomplete, the machine learning model will learn poorly and make bad predictions.\n",
            "*   **Not Always Right:** ML models make \"predictions\" or \"guesses.\" They aren't 100% accurate, especially with new, unusual data.\n",
            "*   **Bias:** If the data used to train the model is biased (e.g., only pictures of one gender or race), the model will learn that bias and perform poorly or unfairly for others.\n",
            "*   **Not \"Conscious\":** Even though they \"learn,\" machine learning models don't *understand* in the way humans do. They are incredibly complex pattern-matching machines.\n",
            "\n",
            "In short, machine learning lets computers get really good at specific tasks by showing them lots of examples and letting them figure out the patterns themselves, rather than us having to write down every single rule. It's about teaching computers to be smart based on experience.\n"
          ]
        }
      ],
      "source": [
        "# TODO:\n",
        "prompt = \"Create a short poem about a cute robot stuck on Mars, making the most of the bleak situation.\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt\n",
        ")\n",
        "\n",
        "print(\"Response from Gemini:\")\n",
        "print(response.text)\n",
        "\n",
        "prompt = \"Explain machine learning in simple terms. Include all relevant detail, but explain to a high schooler with no prior knowledge.\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt\n",
        ")\n",
        "\n",
        "print(\"Response from Gemini:\")\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "148c83ca",
      "metadata": {
        "id": "148c83ca"
      },
      "source": [
        "## 2. Understanding and Counting Tokens\n",
        "\n",
        "Tokens are the basic units that Gemini models use to process text. Understanding token usage is crucial for:\n",
        "- **Cost management**: Billing is based on token consumption\n",
        "- **Context limits**: Models have maximum token limits (e.g., 1M tokens for Gemini 2.5 Pro)\n",
        "- **Performance optimization**: Smaller inputs generally process faster\n",
        "\n",
        "For Gemini models, a token is equivalent to about 4 characters, and 100 tokens equals about 60-80 English words.\n",
        "\n",
        "### Count tokens before generation\n",
        "\n",
        "You can count tokens in your input before sending it to the model to estimate costs and ensure you stay within limits:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ac574ed",
      "metadata": {
        "id": "5ac574ed"
      },
      "outputs": [],
      "source": [
        "prompt = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "# Count tokens in the input\n",
        "# TODO: Call the client.models.count_tokens() method.\n",
        "# Make sure to pass the MODEL_ID and the prompt.\n",
        "# token_count = client.models.count_tokens(\n",
        "#     model=...,\n",
        "#     contents=...\n",
        "# )\n",
        "print(f\"Input tokens: {token_count.total_tokens}\")\n",
        "\n",
        "# Estimate cost (example pricing - check current rates)\n",
        "estimated_cost = token_count.total_tokens * 0.15 / 1_000_000\n",
        "print(f\"Estimated input cost: ${estimated_cost:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0b222b5",
      "metadata": {
        "id": "b0b222b5"
      },
      "source": [
        "### Count tokens after generation\n",
        "\n",
        "After generating content, you can access detailed token usage information:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "644befed",
      "metadata": {
        "id": "644befed"
      },
      "outputs": [],
      "source": [
        "prompt = \"Write a haiku about artificial intelligence.\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt\n",
        ")\n",
        "\n",
        "print(f\"Generated haiku:\\n{response.text}\\n\")\n",
        "\n",
        "# Access token usage metadata\n",
        "usage = response.usage_metadata\n",
        "print(f\"Input tokens: {usage.prompt_token_count}\")\n",
        "print(f\"Thought tokens: {usage.thoughts_token_count}\")\n",
        "print(f\"Output tokens: {usage.candidates_token_count}\")\n",
        "\n",
        "# Calculate total estimated cost\n",
        "total_cost = (usage.prompt_token_count * 0.15 + (usage.candidates_token_count + usage.thoughts_token_count) * 3.5) / 1_000_000\n",
        "print(f\"Total estimated cost: ${total_cost:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bebb35a9",
      "metadata": {
        "id": "bebb35a9"
      },
      "source": [
        "## 3. Text Understanding with `contents`\n",
        "\n",
        "The simplest way to generate text is to provide the model with a text-only prompt. `contents` can be a single prompt, a list of prompts, or a combination of multimodal inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72c5ef98",
      "metadata": {
        "id": "72c5ef98"
      },
      "outputs": [],
      "source": [
        "response_capital = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"What is the capital of France?\"\n",
        ")\n",
        "print(f\"Q: What is the capital of France?\\nA: {response_capital.text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e19e0c0e",
      "metadata": {
        "id": "e19e0c0e"
      },
      "outputs": [],
      "source": [
        "# TODO: Call the client.models.generate_content() method.\n",
        "# For the contents, provide a list of strings:\n",
        "# 1. \"Create 3 names for a vegan restaurant\"\n",
        "# 2. \"city: Berlin\"\n",
        "# response_restaurant_berlin = client.models.generate_content(\n",
        "#     model=MODEL_ID,\n",
        "#     contents=[...]\n",
        "# )\n",
        "print(f\"\\nVegan restaurant names in Berlin:\\n{response_restaurant_berlin.text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "184d9989",
      "metadata": {
        "id": "184d9989"
      },
      "source": [
        "## 4. Streaming Responses\n",
        "\n",
        "Streaming allows you to receive responses incrementally as they're generated, providing a better user experience for long responses or real-time applications like chatbots.\n",
        "\n",
        "**When to use streaming:**\n",
        "- Interactive applications (chatbots, assistants)\n",
        "- Long content generation\n",
        "- Real-time user feedback\n",
        "- Improved perceived performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bfe73e5",
      "metadata": {
        "id": "4bfe73e5"
      },
      "outputs": [],
      "source": [
        "prompt_long_story = \"Write a short story about a brave knight and a friendly dragon.\"\n",
        "\n",
        "print(\"Streaming response:\")\n",
        "for chunk in client.models.generate_content_stream(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt_long_story\n",
        "):\n",
        "    if chunk.text:  # Check if chunk has text content\n",
        "        print(chunk.text, end=\"\", flush=True)\n",
        "print(\"\\n\")  # Add newline at the end"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f9754bc",
      "metadata": {
        "id": "8f9754bc"
      },
      "source": [
        "## 5. Chat (Multi-turn Conversations)\n",
        "\n",
        "The SDK chat class provides an interface to keep track of conversation history. Behind the scenes it uses the same `generate_content` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "845e6896",
      "metadata": {
        "id": "845e6896"
      },
      "outputs": [],
      "source": [
        "chat_session = client.chats.create(model=MODEL_ID)\n",
        "\n",
        "user_message1 = \"I'm planning a weekend trip. Any suggestions for a city break in Europe?\"\n",
        "print(f\"User: {user_message1}\")\n",
        "response1 = chat_session.send_message(message=user_message1)\n",
        "print(f\"Model: {response1.text}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7f88e29",
      "metadata": {
        "id": "b7f88e29"
      },
      "outputs": [],
      "source": [
        "user_message2 = \"I like history and good food. Not too expensive.\"\n",
        "print(f\"User: {user_message2}\")\n",
        "# TODO: Call the chat_session.send_message() method with user_message2.\n",
        "# response2 = chat_session.send_message(message=...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e3a5c84",
      "metadata": {
        "id": "0e3a5c84"
      },
      "outputs": [],
      "source": [
        "# View conversation history\n",
        "history = chat_session.get_history()\n",
        "print(f\"Total messages in conversation: {len(history)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e0be978",
      "metadata": {
        "id": "8e0be978"
      },
      "source": [
        "## 6. System Instructions\n",
        "\n",
        "System instructions let you define the model's behavior and personality. They're applied consistently throughout the conversation.\n",
        "\n",
        "**Best practices for system instructions:**\n",
        "- Be specific and clear\n",
        "- Define the role and tone\n",
        "- Include formatting preferences\n",
        "- Set behavioral guidelines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23844462",
      "metadata": {
        "id": "23844462"
      },
      "outputs": [],
      "source": [
        "system_instruction_poet = \"You are a renowned poet from the 17th century, specializing in sonnets. Respond in iambic pentameter and use eloquent, period-appropriate language.\"\n",
        "\n",
        "response_poet = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"What are your thoughts on modern technology?\",\n",
        "    config=types.GenerateContentConfig(\n",
        "        system_instruction=system_instruction_poet\n",
        "    )\n",
        ")\n",
        "print(f\"\\nPoet model on modern tech:\\n{response_poet.text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09954e6a",
      "metadata": {
        "id": "09954e6a"
      },
      "source": [
        "## 7. Generation Configuration\n",
        "\n",
        "Customize the generation behavior using configuration parameters. Understanding these helps you fine-tune responses for your specific use case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "289d6553",
      "metadata": {
        "id": "289d6553"
      },
      "outputs": [],
      "source": [
        "# Configuration using dictionary\n",
        "generation_config_dict = {\n",
        "    \"temperature\": 0.2,      # Lower = more deterministic, higher = more creative\n",
        "    \"max_output_tokens\": 2000, # Limit response length\n",
        "    \"top_p\": 0.8,            # Nucleus sampling - diversity of token selection\n",
        "    \"top_k\": 30,             # Consider top 30 most likely tokens\n",
        "\n",
        "}\n",
        "\n",
        "# TODO: Call client.models.generate_content()\n",
        "# Pass the MODEL_ID, a prompt to \"Write a very short tagline for a new brand of eco-friendly sneakers.\",\n",
        "# and the generation_config_dict.\n",
        "# response_config = client.models.generate_content(\n",
        "#     model=...,\n",
        "#     contents=...,\n",
        "#     config=...\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59d91651",
      "metadata": {
        "id": "59d91651"
      },
      "source": [
        "**Parameter Guide:**\n",
        "- **Temperature (0.0-2.0)**: Controls randomness. Use 0.2-0.4 for factual content, 0.7-1.0 for creative content\n",
        "- **Top-p (0.0-1.0)**: Controls diversity. Lower values = more focused, higher = more diverse\n",
        "- **Top-k**: Limits token choices. Lower = more focused, higher = more diverse\n",
        "- **Max output tokens**: Prevents overly long responses and controls costs\n",
        "\n",
        "## 8. Long Context and File Uploads\n",
        "\n",
        "Gemini 2.5 Pro has a 1M token context window. In practice, 1 million tokens could look like:\n",
        "\n",
        "- 50,000 lines of code (with the standard 80 characters per line)\n",
        "- All the text messages you have sent in the last 5 years\n",
        "- 8 average length English novels\n",
        "- 1 hour of video data\n",
        "\n",
        "The File API allows you to upload files to the Gemini API and use them as context for your requests."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0964dc9",
      "metadata": {
        "id": "c0964dc9"
      },
      "outputs": [],
      "source": [
        "# Example with a text file (more reliable than the audio example)\n",
        "import requests\n",
        "\n",
        "# Download a sample text file\n",
        "sample_text_url = \"https://www.gutenberg.org/files/74/74-0.txt\"  # Adventures of Tom Sawyer\n",
        "response_req = requests.get(sample_text_url)\n",
        "\n",
        "# Save to local file\n",
        "with open(\"sample_book.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(response_req.text)\n",
        "\n",
        "# Upload the file to the Gemini API\n",
        "try:\n",
        "    myfile = client.files.upload(file=\"sample_book.txt\")\n",
        "    print(f\"File uploaded successfully: {myfile.name}\")\n",
        "\n",
        "    # Generate content using the uploaded file as context\n",
        "    response = client.models.generate_content(\n",
        "        model=MODEL_ID,\n",
        "        contents=[myfile, \"Summarize this book in 3 key points\"]\n",
        "    )\n",
        "\n",
        "    print(\"Summary:\")\n",
        "    print(response.text)\n",
        "\n",
        "    # Check token usage for the large context\n",
        "    print(f\"\\nToken usage: {response.usage_metadata.total_token_count}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error uploading file: {e}\")\n",
        "    print(\"Make sure the file exists and is accessible\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c465f36e",
      "metadata": {
        "id": "c465f36e"
      },
      "source": [
        "## 9. !! Exercise: Chat with a \"Book\" !!\n",
        "\n",
        "Create an interactive chat session where you can \"talk\" to the book \"Alice in Wonderland\". You'll set up the chat with a specific persona for the AI and use the book's text as context for the conversation.\n",
        "\n",
        "Task:\n",
        "- Download the text of \"Alice in Wonderland\" (a helper code block is provided).\n",
        "- Upload the book's text file (`alice_in_wonderland.txt`) to the Gemini API using `client.files.upload()`.\n",
        "- Create a chat session using `client.chats.create()`:\n",
        "- Send an initial message to the chat session using `chat.send_message()`:\n",
        "- Send at least one follow-up question to the chat session (e.g., \"Explain the various methods of speech delivery in more detail\") and print its response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d22740e2",
      "metadata": {
        "id": "d22740e2"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "# Download Alice in Wonderland\n",
        "book_text_url = \"https://www.gutenberg.org/files/11/11-0.txt\"\n",
        "try:\n",
        "    response_book_req = requests.get(book_text_url)\n",
        "    response_book_req.raise_for_status()  # Raise an exception for bad status codes\n",
        "\n",
        "    with open(\"alice_in_wonderland.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(response_book_req.text)\n",
        "    print(\"Book downloaded successfully!\")\n",
        "\n",
        "except requests.RequestException as e:\n",
        "    print(f\"Error downloading book: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34b5b3bc",
      "metadata": {
        "id": "34b5b3bc"
      },
      "outputs": [],
      "source": [
        "# TODO:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c152d709",
      "metadata": {
        "id": "c152d709"
      },
      "outputs": [],
      "source": [
        "# TODO:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76159ebc",
      "metadata": {
        "id": "76159ebc"
      },
      "source": [
        "## Recap & Next Steps\n",
        "\n",
        "**What You've Learned:**\n",
        "- Basic text generation with `client.models.generate_content()` for single prompts\n",
        "- Token counting and cost estimation for better resource management\n",
        "- Streaming responses with `generate_content_stream()` for improved user experience\n",
        "- Multi-turn conversations using `client.chats.create()` and chat sessions\n",
        "- System instructions for consistent model behavior and personality\n",
        "- Generation configuration parameters for fine-tuning responses\n",
        "- Long context handling and file uploads with the File API\n",
        "- Error handling and best practices for production applications\n",
        "\n",
        "**Key Takeaways:**\n",
        "- Monitor token usage to control costs and stay within limits\n",
        "- Use streaming for interactive applications and long responses\n",
        "- Configure parameters based on your use case (factual vs creative content)\n",
        "- Implement proper error handling for robust applications\n",
        "- System instructions are powerful for setting behavior and tone\n",
        "\n",
        "**Next Steps:** Continue with [Part 2: Multimodal Capabilities](https://github.com/philschmid/gemini-2.5-ai-engineering-workshop/blob/main/notebooks/02-multimodal-capabilities.ipynb) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/philschmid/gemini-2.5-ai-engineering-workshop/blob/main/notebooks/02-multimodal-capabilities.ipynb)\n",
        "\n",
        "**More Resources:**\n",
        "- [Text Generation Guide](https://ai.google.dev/gemini-api/docs/text-generation)\n",
        "- [Token Counting Guide](https://ai.google.dev/gemini-api/docs/tokens)\n",
        "- [Long Context Documentation](https://ai.google.dev/gemini-api/docs/long-context)\n",
        "- [File API Documentation](https://ai.google.dev/gemini-api/docs/files)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}